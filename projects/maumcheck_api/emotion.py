from flask import Flask
from flask_cors import CORS


import re
import torch
import torch.nn.functional as F
from flask import request, jsonify


import logging
import os



app = Flask(__name__)


# # ì½”ë“œ ë‚´ì— ëª…ì‹œì  ë¡œê¹…
app.logger.info("emotion request!")


# # ğŸ”¸ ë¡œê·¸ í´ë” ì—†ìœ¼ë©´ ìƒì„±
# if not os.path.exists("log"):
#     os.makedirs("log")

# # ğŸ”¸ ë¡œê·¸ í¬ë§· ì„¤ì •
# log_formatter = logging.Formatter(
#     '[%(asctime)s] %(levelname)s in %(module)s: %(message)s'
# )

# # âœ… 1. íŒŒì¼ í•¸ë“¤ëŸ¬
# file_handler = logging.FileHandler("log/emotion.log", encoding='utf-8')
# file_handler.setFormatter(log_formatter)
# file_handler.setLevel(logging.INFO)

# # âœ… 2. ì½˜ì†” í•¸ë“¤ëŸ¬
# console_handler = logging.StreamHandler()
# console_handler.setFormatter(log_formatter)
# console_handler.setLevel(logging.INFO)

# # ğŸ”¸ Flask ê¸°ë³¸ ë¡œê±°ì— í•¸ë“¤ëŸ¬ ë“±ë¡
# app.logger.setLevel(logging.INFO)
# app.logger.addHandler(file_handler)
# app.logger.addHandler(console_handler)


CORS(app)  # ì „ì²´ ë„ë©”ì¸ í—ˆìš©
#CORS(app, resources={r"/emotion": {"origins": "http://vb901217.dothome.co.kr"}})


@app.route('/')
def hello():
    return 'hello'

import torch
import torch.nn as nn
from transformers import AutoTokenizer, ElectraModel


# ê°ì • ë¶„ë¥˜ ëª¨ë¸ ì •ì˜
class KoELECTRAEmotion(nn.Module):
    def __init__(self, model_name, category=12, dropout_prob=0.1):
        super(KoELECTRAEmotion, self).__init__()
        self.electra = ElectraModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(dropout_prob)
        self.fc = nn.Linear(768, 256)
        self.relu = nn.ReLU()
        self.emotion_classifier = nn.Linear(256, category)

    def forward(self, input_ids, attention_mask, token_type_ids=None):
        outputs = self.electra(
            input_ids=input_ids, 
            attention_mask=attention_mask, 
            token_type_ids=token_type_ids,
            return_dict=True,
            output_attentions=True
        )

        self.last_attention = outputs.attentions
        cls_output = outputs.last_hidden_state[:, 0, :]
        x = self.fc(cls_output)
        x = self.relu(x)
        x = self.dropout(x)
        category_logits = self.emotion_classifier(x)

        return category_logits


# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device = torch.device("cpu")

model_name = "monologg/koelectra-base-discriminator"  # âœ… KoELECTRA ì ìš©
model = KoELECTRAEmotion(model_name, category=12)  # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
model.load_state_dict(torch.load("model/emotion_model.pth", map_location=device))
model.to(device)
model.eval()

# âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(model_name)

# emotion_mapping = {
#     0: ('EM01', 'ê¸°ì¨'), 1: ('EM02', 'ìŠ¬í””'), 2: ('EM03', 'ë¶„ë…¸'), 3: ('EM04', 'í˜ì˜¤'),
#     4: ('EM05', 'ë¶ˆì•ˆ'), 5: ('EM06', 'ê³µí¬'), 6: ('EM07', 'ë†€ëŒ'), 7: ('EM08', 'ìˆ˜ì¹˜ì‹¬'),
#     8: ('EM09', 'ì ˆë§ê°'), 9: ('EM10', 'ì£„ì±…ê°'), 10: ('EM11', 'ìš°ìš¸/ì™¸ë¡œì›€'), 11: ('EM12', 'ê¸°íƒ€')
# }


emotion_mapping = {
    0: "ê¸°ì¨", 
    1: "ìŠ¬í””", 
    2: "ë¶„ë…¸", 
    3: "ë¶ˆì•ˆ",
    4: "ì£„ì±…ê°", 
    5: "ìš°ìš¸", 
    6: "ë‹¹í™©", 
    7: "í˜ì˜¤",
    8: "ë†€ëŒ", 
    9: "ì™¸ë¡œì›€", 
    10: "ë¬´ê¸°ë ¥", 
    11: "ê¸°íƒ€"
}

situation_mapping = {
    0: 'ê°€ì¡± ë° ëŒ€ì¸ê´€ê³„', 
    1: 'í•™ì—… ë° ì§„ë¡œ',
    2: 'ì§ì¥ ë° ì—…ë¬´ ìŠ¤íŠ¸ë ˆìŠ¤',
    3: 'ì •ì‹  ê±´ê°• ë¬¸ì œ', 
    4: 'í•™êµí­ë ¥ ë° í•™ëŒ€',
    5: 'ì¤‘ë… ë¬¸ì œ',
    6: 'ê²½ì œì  ì–´ë ¤ì›€',
    7: 'ì‹ ì²´ ê±´ê°• ë¬¸ì œ', 
    8: 'ì£½ìŒê³¼ ìƒì‹¤',
    9: 'ì¸ì§€ ê¸°ëŠ¥ ê´€ë ¨ ì–´ë ¤ì›€', 
    10: 'ë¯¼ê°í•œ í”¼í•´ ê²½í—˜', 
    11: 'ì—°ì• /ê²°í˜¼/ì¶œì‚°'
}




@app.route("/emotion", methods=["POST"])
def predict_emotion():
    try:
        data = request.get_json()
        if not data or "text" not in data:
            return jsonify({'error': 'No text provided'}), 400

        # í…ìŠ¤íŠ¸ ì •ë¦¬
        text = re.sub(r"<.*?>", "", data["text"]).strip()
        text = preprocess_text(text)
        app.logger.debug(f"[DEBUG] text: {text}")
        if len(text) < 5:
            return jsonify({
                "input_text": text,
                "top3_predictions": [{"emotion": "ë¶„ì„ë¶ˆê°€", "probability": 1.0}],
                "keywords": []
            })

        # Tokenization
        encoding = tokenizer(text, return_tensors="pt", truncation=False)
        input_ids = encoding["input_ids"][0]
        attention_mask = encoding["attention_mask"][0]
        token_type_ids = encoding.get("token_type_ids", torch.zeros_like(input_ids))

        # Chunking
        def pad_and_chunk(input_ids, attention_mask, token_type_ids, start, max_len):
            end = min(start + max_len, len(input_ids))
            chunk_input_ids = input_ids[start:end].unsqueeze(0)
            chunk_attention_mask = attention_mask[start:end].unsqueeze(0)

            if token_type_ids.dim() == 1:
                chunk_token_type_ids = token_type_ids[start:end].unsqueeze(0)
            else:
                chunk_token_type_ids = token_type_ids[:, start:end]

            pad_len = max_len - chunk_input_ids.size(1)
            if pad_len > 0:
                pad = torch.zeros((1, pad_len), dtype=torch.long)
                chunk_input_ids = torch.cat([chunk_input_ids, pad], dim=1)
                chunk_attention_mask = torch.cat([chunk_attention_mask, pad], dim=1)
                chunk_token_type_ids = torch.cat([chunk_token_type_ids, pad], dim=1)

            return {
                "input_ids": chunk_input_ids,
                "attention_mask": chunk_attention_mask,
                "token_type_ids": chunk_token_type_ids
            }

        MAX_LEN, STRIDE = 512, 256
        chunks = [
            pad_and_chunk(input_ids, attention_mask, token_type_ids, start, MAX_LEN)
            for start in range(0, len(input_ids), STRIDE)
        ]

        if not chunks:
            return jsonify({
                "input_text": text,
                "top3_predictions": [{"emotion": "ê¸°íƒ€", "probability": 1.0}],
                "keywords": []
            })

        total_probs = torch.zeros(len(emotion_mapping)).to(device)
        keywords_all = []

        for chunk in chunks:
            tokens = {k: v.to(device) for k, v in chunk.items()}

            with torch.no_grad():
                logits = model(**tokens)
                probs = F.softmax(logits, dim=1)[0]
                total_probs += probs

                # ğŸ” ì¤‘ì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
                try:
                    attn = model.last_attention[-1][0]
                    cls_att = attn[0]
                    token_strs = tokenizer.convert_ids_to_tokens(tokens["input_ids"][0])

                    if cls_att.shape[0] == len(token_strs):
                        token_scores = cls_att[1:].mean(dim=0)
                        keyword_scores = list(zip(token_strs[1:], token_scores.tolist()))
                        keyword_scores.sort(key=lambda x: x[1], reverse=True)

                        merged = merge_wordpieces(keyword_scores)
                        merged.sort(key=lambda x: x[1], reverse=True)

                        app.logger.debug(f"[DEBUG] merged tokens: {merged[:10]}")

                        if merged:
                            top_tokens = [token for token, _ in merged[:5]]
                            for core_token in top_tokens:
                                context = extract_context_window(text, core_token, window_size=30)
                                app.logger.debug(f"[DEBUG] context for '{core_token}': {context}")
                                context_keywords = extract_keywords_from_text(context)
                                keywords_all.extend(context_keywords)
                except Exception as ke:
                    app.logger.warning(f"[í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜] {str(ke)}")

        avg_probs = total_probs / len(chunks)
        top3_probs, top3_indices = torch.topk(avg_probs, k=3)

        top3_results = [
            {
                "emotion": emotion_mapping.get(idx.item(), f"ë¶„ë¥˜ë¶ˆê°€({idx.item()})"),
                "probability": round(prob.item(), 4)
            }
            for idx, prob in zip(top3_indices, top3_probs)
        ]

        # unique_keywords = list(dict.fromkeys(keywords_all))[:20]
        # if not unique_keywords:
        #     unique_keywords = ["(í‚¤ì›Œë“œ ì—†ìŒ)"]

        unique_keywords = filter_final_keywords(keywords_all, top_k=10)
        if not unique_keywords:
            unique_keywords = ["(í‚¤ì›Œë“œ ì—†ìŒ)"]

        return jsonify({
            "input_text": text,
            "top3_predictions": top3_results,
            "keywords": unique_keywords
        })

    except Exception as e:
        app.logger.error(f"[ERROR] ê°ì • ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        return jsonify({'error': f"ì„œë²„ ì˜¤ë¥˜: {str(e)}"}), 500
















def preprocess_text(text):
    important_chars = "!?,~ã… ã…‹^â€¦"
    text = ''.join([char if char in important_chars or char.isalnum() else ' ' for char in text])
    text = re.sub(r'([!?.])\1+', r'\1', text)
    text = re.sub(r'(ã…‹)\1+', r'\1\1', text)
    text = re.sub(r'(ã… )\1+', r'\1\1', text)
    return text.strip()



# WordPiece í† í°ì„ ê²°í•©í•˜ì—¬ ì›ë˜ ë‹¨ì–´ í˜•íƒœë¡œ ë³µì›
def merge_wordpieces(token_scores):
    merged_tokens = []
    current_word = ""
    current_score = 0.0
    count = 0
    for token, score in token_scores:
        if token.startswith("##"):
            current_word += token[2:]
            current_score += score
            count += 1
        else:
            if current_word:
                merged_tokens.append((current_word, current_score / count))
            current_word = token
            current_score = score
            count = 1
    if current_word:
        merged_tokens.append((current_word, current_score / count))
    return merged_tokens





from konlpy.tag import Okt
from kiwipiepy import Kiwi
from collections import OrderedDict

okt = Okt()
kiwi = Kiwi()



#  ì£¼ì–´ì§„ í† í°ì´ í¬í•¨ëœ ë¬¸ì¥ì„ ì›ë¬¸ì—ì„œ ì°¾ì•„ ë°˜í™˜

def find_sentence_with_token(token, text):
    token_clean = token.replace("##", "")
    sentences = re.split(r'[.!?\n]', text)  # ë¬¸ì¥ ë¶„ë¦¬

    for sent in sentences:
        if token_clean in sent:
            return sent.strip()
    return token_clean  # fallback: í† í° ìì²´



# í•´ë‹¹ ë¬¸ì¥ì—ì„œ Oktì™€ Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ í™œìš©í•˜ì—¬ ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ ì¶”ì¶œ
def extract_keywords_from_text(text):
    keywords = set()
    try:
        okt_tokens = okt.pos(text, stem=True)
        keywords.update([word for word, tag in okt_tokens if tag in ["Noun", "Verb", "Adjective"] and len(word) >= 2])
    except Exception as e:
        app.logger.warning(f"[Okt ë¶„ì„ ì˜¤ë¥˜] {str(e)}")

    try:
        kiwi_tokens = kiwi.analyze(text)
        if kiwi_tokens:
            for token in kiwi_tokens[0][0]:
                if token.tag in ["NNG", "NNP", "VV", "VA"] and len(token.form) >= 2:
                    keywords.add(token.form)
    except Exception as e:
        app.logger.warning(f"[Kiwi ë¶„ì„ ì˜¤ë¥˜] {str(e)}")

    return list(keywords)



def extract_context_window(text, token, window_size=30):
    token_clean = token.replace("##", "")
    idx = text.find(token_clean)
    if idx == -1:
        return token_clean
    start = max(0, idx - window_size)
    end = min(len(text), idx + len(token_clean) + window_size)
    return text[start:end]


def filter_final_keywords(keywords, top_k=10):
    allowed_tags = {"Noun", "Verb", "Adjective"}
    seen = set()
    filtered = []

    for kw in keywords:
        if len(kw) < 2:  # ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ ì œì™¸
            continue
        if kw in seen:
            continue
        seen.add(kw)
        filtered.append(kw)
        if len(filtered) >= top_k:
            break

    return filtered



# from collections import OrderedDict
# from kiwipiepy import Kiwi
# from konlpy.tag import Okt

# kiwi = Kiwi()
# okt = Okt()

# def find_sentence_with_token(token, text):
#     for sentence in text.split('\n'):
#         if token.replace("##", "") in sentence:
#             return sentence
#     return text

# def extract_keywords_from_top_tokens(text, top_tokens, top_k=3):
#     keywords = []
#     for token in top_tokens:
#         sentence = find_sentence_with_token(token, text)
#         morphs = extract_keywords_from_text(sentence)
#         keywords.extend(morphs)
#     unique_keywords = list(OrderedDict.fromkeys(keywords))
#     return unique_keywords[:top_k]

# def extract_keywords_from_text(text):
#     tokens = okt.pos(text, stem=True)
#     return [word for word, tag in tokens if tag in ["Noun", "Verb", "Adjective"] and len(word) >= 2]


# def merge_wordpieces(token_scores):
#     merged_tokens = []
#     current_word = ""
#     current_score = 0.0
#     count = 0

#     for token, score in token_scores:
#         if token.startswith("##"):
#             current_word += token[2:]
#             current_score += score
#             count += 1
#         else:
#             if current_word:
#                 merged_tokens.append((current_word, current_score / count))
#             current_word = token
#             current_score = score
#             count = 1

#     # ë§ˆì§€ë§‰ ë‹¨ì–´ ì²˜ë¦¬
#     if current_word:
#         merged_tokens.append((current_word, current_score / count))

#     # [PAD], [SEP] ê°™ì€ ë¬´ì˜ë¯¸ í† í° ì œê±°
#     merged_tokens = [(w, s) for w, s in merged_tokens if w not in ['[PAD]', '[SEP]', '[CLS]']]

#     return merged_tokens




if __name__ == "__main__":
    app.run(host="127.0.0.1", port=5001, debug=True)


# if __name__ == "__main__":
#     app.run(host="0.0.0.0", port=5001, debug=False, use_reloader=False)


# if __name__ == "__main__":
#     debug_mode = os.environ.get("DEBUG_MODE", "True") == "True"
#     app.run(
#         host="0.0.0.0" if not debug_mode else "127.0.0.1",
#         port=5001,
#         debug=debug_mode,
#         use_reloader=debug_mode
#     )

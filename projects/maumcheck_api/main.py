"""
üî∏ ÌÜµÌï© Flask ÏÑúÎ≤Ñ
- /emotion ‚Üí Í∞êÏ†ï ÏòàÏ∏°
- /situation ‚Üí ÏÉÅÌô© Î∂ÑÎ•ò
"""

# ‚úÖ Í≥µÌÜµ ÎùºÏù¥Î∏åÎü¨Î¶¨ Î∞è ÏÑ§Ï†ï
from flask import Flask, request, jsonify
from flask_cors import CORS

import torch
import torch.nn.functional as F
import torch.nn as nn
from transformers import AutoTokenizer, ElectraModel

import re
import logging
import os

from konlpy.tag import Okt
from kiwipiepy import Kiwi

# ‚úÖ Flask Ïï± Ï†ïÏùò
app = Flask(__name__)


#

allowed_origins = [
    "http://vb901217.dothome.co.kr",
    "http://maumcheck.site"
]

#CORS(app)

CORS(app, resources={

    r"/emotion": {"origins": allowed_origins},
    r"/situation": {"origins": allowed_origins},

})

# ‚úÖ ÌòïÌÉúÏÜå Î∂ÑÏÑùÍ∏∞ Ï¥àÍ∏∞Ìôî
okt = Okt()
kiwi = Kiwi()

# ‚úÖ ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï
device = torch.device("cpu")

# ‚úÖ KoELECTRA Í≥µÌÜµ ÏÑ§Ï†ï
model_name = "monologg/koelectra-base-discriminator"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# --------------------------------------------
# üîπ Í∞êÏ†ï Î∂ÑÎ•ò Î™®Îç∏ Ï†ïÏùò Î∞è Î°úÎìú
# --------------------------------------------
class KoELECTRAEmotion(nn.Module):
    """KoELECTRA Í∏∞Î∞ò Í∞êÏ†ï Î∂ÑÎ•ò Î™®Îç∏"""
    def __init__(self, model_name, category=12, dropout_prob=0.1):
        super(KoELECTRAEmotion, self).__init__()
        self.electra = ElectraModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(dropout_prob)
        self.fc = nn.Linear(768, 256)
        self.relu = nn.ReLU()
        self.emotion_classifier = nn.Linear(256, category)

    def forward(self, input_ids, attention_mask, token_type_ids=None):
        outputs = self.electra(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            return_dict=True,
            output_attentions=True
        )
        self.last_attention = outputs.attentions
        cls_output = outputs.last_hidden_state[:, 0, :]
        x = self.fc(cls_output)
        x = self.relu(x)
        x = self.dropout(x)
        return self.emotion_classifier(x)

emotion_model = KoELECTRAEmotion(model_name, category=12)
emotion_model.load_state_dict(torch.load("model/emotion_model.pth", map_location=device))
emotion_model.to(device)
emotion_model.eval()

emotion_mapping = {
    0: "Í∏∞ÏÅ®", 1: "Ïä¨Ìîî", 2: "Î∂ÑÎÖ∏", 3: "Î∂àÏïà",
    4: "Ï£ÑÏ±ÖÍ∞ê", 5: "Ïö∞Ïö∏", 6: "ÎãπÌô©", 7: "ÌòêÏò§",
    8: "ÎÜÄÎûå", 9: "Ïô∏Î°úÏõÄ", 10: "Î¨¥Í∏∞Î†•", 11: "Í∏∞ÌÉÄ"
}

# --------------------------------------------
# üîπ ÏÉÅÌô© Î∂ÑÎ•ò Î™®Îç∏ Ï†ïÏùò Î∞è Î°úÎìú
# --------------------------------------------
class KoELECTRASituation(nn.Module):
    """KoELECTRA Í∏∞Î∞ò ÏÉÅÌô© Î∂ÑÎ•ò Î™®Îç∏"""
    def __init__(self, model_name, category=12, dropout_prob=0.1):
        super(KoELECTRASituation, self).__init__()
        self.electra = ElectraModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(dropout_prob)
        self.fc = nn.Linear(768, 256)
        self.relu = nn.ReLU()
        self.situation_classifier = nn.Linear(256, category)

    def forward(self, input_ids, attention_mask, token_type_ids=None):
        outputs = self.electra(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            return_dict=True,
            output_attentions=True
        )
        self.last_attention = outputs.attentions
        cls_output = outputs.last_hidden_state[:, 0, :]
        x = self.fc(cls_output)
        x = self.relu(x)
        x = self.dropout(x)
        return self.situation_classifier(x)

situation_model = KoELECTRASituation(model_name, category=12)
situation_model.load_state_dict(torch.load("model/situation_model.pth", map_location=device))
situation_model.to(device)
situation_model.eval()

situation_mapping = {
    0: 'Í∞ÄÏ°± Î∞è ÎåÄÏù∏Í¥ÄÍ≥Ñ', 1: 'ÌïôÏóÖ Î∞è ÏßÑÎ°ú', 2: 'ÏßÅÏû• Î∞è ÏóÖÎ¨¥ Ïä§Ìä∏Î†àÏä§',
    3: 'Ï†ïÏã† Í±¥Í∞ï Î¨∏Ï†ú', 4: 'ÌïôÍµêÌè≠Î†• Î∞è ÌïôÎåÄ', 5: 'Ï§ëÎèÖ Î¨∏Ï†ú',
    6: 'Í≤ΩÏ†úÏ†Å Ïñ¥Î†§ÏõÄ', 7: 'Ïã†Ï≤¥ Í±¥Í∞ï Î¨∏Ï†ú', 8: 'Ï£ΩÏùåÍ≥º ÏÉÅÏã§',
    9: 'Ïù∏ÏßÄ Í∏∞Îä• Í¥ÄÎ†® Ïñ¥Î†§ÏõÄ', 10: 'ÎØºÍ∞êÌïú ÌîºÌï¥ Í≤ΩÌóò', 11: 'Ïó∞Ïï†/Í≤∞Ìòº/Ï∂úÏÇ∞'
}

# --------------------------------------------
# üîπ Í≥µÌÜµ Ìï®Ïàò Ï†ïÏùò (Ï†ÑÏ≤òÎ¶¨, ÌÇ§ÏõåÎìú Ï∂îÏ∂ú Îì±)
# --------------------------------------------
def preprocess_text(text):
    """ÌäπÏàòÎ¨∏Ïûê Ï†úÍ±∞ Î∞è Î∞òÎ≥µ Î¨∏Ïûê Ï†ïÎ¶¨"""
    important_chars = "!?,~„Ö†„Öã^‚Ä¶"
    text = ''.join([char if char in important_chars or char.isalnum() else ' ' for char in text])
    text = re.sub(r'([!?.])\1+', r'\1', text)
    text = re.sub(r'(„Öã)\1+', r'\1\1', text)
    text = re.sub(r'(„Ö†)\1+', r'\1\1', text)
    return text.strip()

def merge_wordpieces(token_scores):
    """WordPiece ÌÜ†ÌÅ∞ÏùÑ Í≤∞Ìï©ÌïòÏó¨ ÏõêÎûò Îã®Ïñ¥ Î≥µÏõê"""
    merged_tokens = []
    current_word = ""
    current_score = 0.0
    count = 0
    for token, score in token_scores:
        if token.startswith("##"):
            current_word += token[2:]
            current_score += score
            count += 1
        else:
            if current_word:
                merged_tokens.append((current_word, current_score / count))
            current_word = token
            current_score = score
            count = 1
    if current_word:
        merged_tokens.append((current_word, current_score / count))
    return merged_tokens

def extract_context_window(text, token, window_size=30):
    """Ìï¥Îãπ ÌÜ†ÌÅ∞Ïù¥ Ìè¨Ìï®Îêú Ï£ºÎ≥Ä Î¨∏Îß• Ï∂îÏ∂ú"""
    token_clean = token.replace("##", "")
    idx = text.find(token_clean)
    if idx == -1:
        return token_clean
    start = max(0, idx - window_size)
    end = min(len(text), idx + len(token_clean) + window_size)
    return text[start:end]

def extract_keywords_from_text(text):
    """Okt Î∞è KiwiÎ°ú Î™ÖÏÇ¨/ÎèôÏÇ¨/ÌòïÏö©ÏÇ¨ Ï∂îÏ∂ú"""
    keywords = set()
    try:
        okt_tokens = okt.pos(text, stem=True)
        keywords.update([word for word, tag in okt_tokens if tag in ["Noun", "Verb", "Adjective"] and len(word) >= 2])
    except:
        pass
    try:
        kiwi_tokens = kiwi.analyze(text)
        for token in kiwi_tokens[0][0]:
            if token.tag in ["NNG", "NNP", "VV", "VA"] and len(token.form) >= 2:
                keywords.add(token.form)
    except:
        pass
    return list(keywords)

def filter_final_keywords(keywords, top_k=10):
    """ÏµúÏ¢Ö ÌÇ§ÏõåÎìú ÌïÑÌÑ∞ÎßÅ (Ï§ëÎ≥µ Ï†úÍ±∞, Í∏∏Ïù¥ Ï†úÌïú Îì±)"""
    seen = set()
    filtered = []
    for kw in keywords:
        if len(kw) < 2 or kw in seen:
            continue
        seen.add(kw)
        filtered.append(kw)
        if len(filtered) >= top_k:
            break
    return filtered





@app.route("/")
def hello():
    return "hello, myModel"


# --------------------------------------------
# üîπ Í∞êÏ†ï ÏòàÏ∏° ÎùºÏö∞Ìä∏
# --------------------------------------------
@app.route("/emotion", methods=["POST"])
def predict_emotion():
    data = request.get_json()
    if not data or "text" not in data:
        return jsonify({'error': 'No text provided'}), 400

    text = preprocess_text(data["text"])
    encoding = tokenizer(text, return_tensors="pt", truncation=False)
    input_ids = encoding["input_ids"][0]
    attention_mask = encoding["attention_mask"][0]
    token_type_ids = encoding.get("token_type_ids", torch.zeros_like(input_ids))

    MAX_LEN, STRIDE = 512, 256
    chunks = [
        {
            "input_ids": input_ids[start:start + MAX_LEN].unsqueeze(0),
            "attention_mask": attention_mask[start:start + MAX_LEN].unsqueeze(0),
            "token_type_ids": token_type_ids[start:start + MAX_LEN]
        }
        for start in range(0, len(input_ids), STRIDE)
    ]

    total_probs = torch.zeros(len(emotion_mapping)).to(device)
    keywords_all = []

    for chunk in chunks:
        tokens = {k: v.to(device) for k, v in chunk.items()}
        with torch.no_grad():
            logits = emotion_model(**tokens)
            probs = F.softmax(logits, dim=1)[0]
            total_probs += probs

            attn = emotion_model.last_attention[-1][0]
            cls_att = attn[0]
            token_strs = tokenizer.convert_ids_to_tokens(tokens["input_ids"][0])
            if cls_att.shape[0] == len(token_strs):
                token_scores = list(zip(token_strs[1:], cls_att[1:].mean(dim=0).tolist()))
                merged = merge_wordpieces(token_scores)
                merged.sort(key=lambda x: x[1], reverse=True)
                for token, _ in merged[:5]:
                    ctx = extract_context_window(text, token)
                    keywords_all.extend(extract_keywords_from_text(ctx))

    avg_probs = total_probs / len(chunks)
    top3_probs, top3_indices = torch.topk(avg_probs, k=3)
    top3_results = [
        {"emotion": emotion_mapping[idx.item()], "probability": round(prob.item(), 4)}
        for idx, prob in zip(top3_indices, top3_probs)
    ]
    return jsonify({
        "input_text": text,
        "top3_predictions": top3_results,
        "keywords": filter_final_keywords(keywords_all)
    })

# --------------------------------------------
# üîπ ÏÉÅÌô© ÏòàÏ∏° ÎùºÏö∞Ìä∏
# --------------------------------------------
@app.route("/situation", methods=["POST"])
def predict_situation():
    data = request.get_json()
    if not data or "text" not in data:
        return jsonify({'error': 'No text provided'}), 400

    text = preprocess_text(data["text"])
    encoding = tokenizer(text, return_tensors="pt", truncation=False)
    input_ids = encoding["input_ids"][0]
    attention_mask = encoding["attention_mask"][0]
    token_type_ids = encoding.get("token_type_ids", torch.zeros_like(input_ids))

    MAX_LEN, STRIDE = 512, 256
    chunks = [
        {
            "input_ids": input_ids[start:start + MAX_LEN].unsqueeze(0),
            "attention_mask": attention_mask[start:start + MAX_LEN].unsqueeze(0),
            "token_type_ids": token_type_ids[start:start + MAX_LEN]
        }
        for start in range(0, len(input_ids), STRIDE)
    ]

    total_probs = torch.zeros(len(situation_mapping)).to(device)
    keywords_all = []

    for chunk in chunks:
        tokens = {k: v.to(device) for k, v in chunk.items()}
        with torch.no_grad():
            logits = situation_model(**tokens)
            probs = F.softmax(logits, dim=1)[0]
            total_probs += probs

            attn = situation_model.last_attention[-1][0]
            cls_att = attn[0]
            token_strs = tokenizer.convert_ids_to_tokens(tokens["input_ids"][0])
            if cls_att.shape[0] == len(token_strs):
                token_scores = list(zip(token_strs[1:], cls_att[1:].mean(dim=0).tolist()))
                merged = merge_wordpieces(token_scores)
                merged.sort(key=lambda x: x[1], reverse=True)
                for token, _ in merged[:5]:
                    ctx = extract_context_window(text, token)
                    keywords_all.extend(extract_keywords_from_text(ctx))

    avg_probs = total_probs / len(chunks)
    top3_probs, top3_indices = torch.topk(avg_probs, k=3)
    top3_results = [
        {"situation": situation_mapping[idx.item()], "probability": round(prob.item(), 4)}
        for idx, prob in zip(top3_indices, top3_probs)
    ]
    return jsonify({
        "input_text": text,
        "top3_predictions": top3_results,
        "keywords": filter_final_keywords(keywords_all)
    })

# --------------------------------------------
# üîπ ÏÑúÎ≤Ñ Ïã§Ìñâ
# --------------------------------------------
# if __name__ == "__main__":
#     app.run(host="0.0.0.0", port=5000, debug=True)

if __name__ == "__main__":
    app.run(host="127.0.0.1", port=5000, debug=True)